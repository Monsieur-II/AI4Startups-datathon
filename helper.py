# -*- coding: utf-8 -*-
"""afrimash recommender.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yQ_kCiDpVg1Ssoa6WLlCTC_yvjGAAmdj
"""

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import MinMaxScaler
import random
from rapidfuzz import process, fuzz
from IPython.display import display, HTML
from PIL import Image
import requests
from io import BytesIO
from fuzzywuzzy import process

# ============================================
# 1. LOAD DATA
# ============================================
transactions = pd.read_excel("recommender_trans_subset_exploded.xlsx")
rfm = pd.read_excel("recommender_RFM_Data.xlsx")

# Clean product text
transactions["Product(s)"] = transactions["Product(s)"].str.replace(r"\d+×", "", regex=True).str.strip()
transactions["Product(s)"] = transactions["Product(s)"].str.replace(r"\s+", " ", regex=True)
transactions = transactions[transactions["Product(s)"].astype(str).str.strip().ne("")].dropna(subset=["Product(s)"])

# ===============================
# LOAD IMAGE DATA
# ===============================
image_map_file = "afrimash_image_map.xlsx"  # the sitemap-extracted excel
image_df = pd.read_excel(image_map_file)

# Keep only rows with valid Image_Title or Page URL
image_df = image_df.dropna(subset=["Image_Name", "Image_URL"])

# Create a list of titles for fuzzy matching
image_titles = image_df["Image_Name"].fillna("").tolist()



# ============================================
# 2. BUILD PRODUCT CONTENT SIMILARITY
# ============================================
# Clean product text
transactions["Product(s)"] = transactions["Product(s)"].astype(str)
transactions["Product(s)"] = transactions["Product(s)"].str.replace(r"\d+×", "", regex=True).str.strip()
transactions["Product(s)"] = transactions["Product(s)"].str.replace(r"\s+", " ", regex=True)

# Drop rows with empty or missing product names
transactions = transactions[transactions["Product(s)"].notna()]
transactions = transactions[transactions["Product(s)"].str.strip() != ""]

# Model
tfidf = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf.fit_transform(transactions["Product(s)"])
cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

# Product lookup
product_indices = pd.Series(transactions.index, index=transactions["Product(s)"]).drop_duplicates()

def get_content_recommendations(product_name, top_n=10):
    # Skip invalid product names
    if pd.isna(product_name) or product_name.strip() == "":
        return []

    # If product not in index, skip
    if product_name not in product_indices:
        return []

    # Get the index (handle duplicates safely)
    idx = product_indices[product_name]
    if isinstance(idx, (pd.Series, list, np.ndarray)):
        idx = idx.iloc[0] if isinstance(idx, pd.Series) else idx[0]

    # Compute similarity
    sim_scores = list(enumerate(cosine_sim[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = sim_scores[1:top_n+1]  # skip the item itself

    # Get product names of top similar items
    product_indices_top = [i[0] for i in sim_scores]
    return transactions.iloc[product_indices_top]["Product(s)"].tolist()

# ============================================
# 3. BUILD COLLABORATIVE FILTERING
# ============================================
# Create product basket per order
basket = transactions.groupby(['Order #'])['Product(s)'].apply(list).reset_index()

# Build co-occurrence matrix
from collections import defaultdict
co_matrix = defaultdict(lambda: defaultdict(int))

for row in basket['Product(s)']:
    for i in range(len(row)):
        for j in range(i+1, len(row)):
            co_matrix[row[i]][row[j]] += 1
            co_matrix[row[j]][row[i]] += 1

def get_collab_recommendations(product_name, top_n=10):
    related = co_matrix.get(product_name, {})
    sorted_related = sorted(related.items(), key=lambda x: x[1], reverse=True)
    return [k for k, _ in sorted_related[:top_n]]

# ============================================
# 4. HYBRID RECOMMENDER
# ============================================
# ============================================
# 4. HYBRID RECOMMENDER (Optimized & Personalized)
# ============================================
def hybrid_recommend(customer_id, top_n=10):
    """
    Returns top N recommended products for a given customer.
    Hybrid approach combining content + collaborative + recency weighting.
    """

    # === 1. Handle case where customer doesn't exist in transactions ===
    if customer_id not in transactions["Customer_ID"].unique():
        popular_products = transactions["Product(s)"].dropna().value_counts().index.tolist()
        if len(popular_products) >= top_n:
            return random.sample(popular_products[:30], k=min(top_n, 10))
        return popular_products[:top_n]

    # === 2. Get customer type and recent purchase history ===
    cust_type = rfm.loc[rfm["Customer_ID"] == customer_id, "Customer_Type"].values
    cust_type = cust_type[0] if len(cust_type) > 0 else "new"

    cust_orders = transactions[transactions["Customer_ID"] == customer_id].copy()
    cust_orders = cust_orders.sort_values("datetime", ascending=False)
    purchased = cust_orders["Product(s)"].dropna().unique()

    # === 3. Compute recency weights (recent products = higher influence) ===
    recency_weights = {
        prod: 1.0 - (i / max(len(cust_orders), 1)) * 0.5
        for i, prod in enumerate(cust_orders["Product(s)"])
    }

    recommendations = {}

    # === 4. Generate product-level recommendations ===
    for prod in purchased:
        if not isinstance(prod, str) or prod.strip().lower() == 'nan':
            continue

        # Content-based + collaborative
        content_recs = get_content_recommendations(prod, top_n)
        collab_recs = get_collab_recommendations(prod, top_n)

        # Clean invalids
        content_recs = [r for r in content_recs if isinstance(r, str) and r.strip().lower() != 'nan']
        collab_recs = [r for r in collab_recs if isinstance(r, str) and r.strip().lower() != 'nan']

        # Weight by customer type
        if cust_type == "new":
            combined = content_recs[:6] + collab_recs[:4]
        else:
            combined = collab_recs[:6] + content_recs[:4]

        # Add recency-based weight
        weight = recency_weights.get(prod, 1.0)
        for r in combined:
            if r not in purchased:
                recommendations[r] = recommendations.get(r, 0) + weight

    # === 5. Sort recommendations by score ===
    ranked = sorted(recommendations.items(), key=lambda x: x[1], reverse=True)
    top_items = [r[0] for r in ranked[:top_n]]

    # === 6. Diversity control: avoid duplicates of same product family ===
    diversified = []
    for item in top_items:
        # Skip duplicates of the same leading keyword
        if not any(item.split()[0].lower() in x.lower() for x in diversified):
            diversified.append(item)
    top_items = diversified[:top_n]

    # === 7. Fallback if not enough unique recommendations ===
    if len(top_items) < top_n:
        popular_products = transactions["Product(s)"].dropna().value_counts().index.tolist()
        for prod in popular_products:
            if prod not in purchased and prod not in top_items:
                top_items.append(prod)
            if len(top_items) >= top_n:
                break

    # === 8. Final cleanup ===
    top_items = [r for r in top_items if isinstance(r, str) and r.strip().lower() != 'nan']

    return top_items[:top_n]

def match_images_to_products(products, image_titles=image_titles, image_df=image_df, top_score_threshold=60):
    """
    Given a list of product names, return a mapping to the most similar image.
    Uses fuzzy text matching.
    """
    results = []
    for product in products:
        if not product or pd.isna(product):
            continue

        # Get the best match from the image titles
        best_match = process.extractOne(
            query=product,
            choices=image_titles,
            scorer=fuzz.token_sort_ratio
        )

        matched_image_url = None
        matched_title = None

        if best_match and best_match[1] >= top_score_threshold:
            matched_title = best_match[0]
            # Get corresponding image URL
            matched_row = image_df[image_df["Image_Name"] == matched_title].iloc[0]
            matched_image_url = matched_row["Image_URL"]

        results.append({
            "Product_Name": product,
            "Matched_Image_Title": matched_title,
            "Matched_Image_URL": matched_image_url
        })
    return results



# # ===============================
# # Step 2: Display in notebook (2x5 grid)
# # ===============================
# # ===============================
# # FALLBACK SETUP
# # ===============================

# image_urls = image_df['Image_URL'].dropna().tolist()
# image_filenames = [url.split('/')[-1].lower() for url in image_urls]

# # Define fallback keywords (expand as needed)
# keywords = ["broiler", "pullet", "pullets", "duckling", "turkey", "papaya", "maize", "snail", "fertilizer", "coccidiosis"]

# keyword_image_map = {}
# for kw in keywords:
#     matches = [url for url, fname in zip(image_urls, image_filenames) if kw in fname]
#     if matches:
#         keyword_image_map[kw] = matches

# def display_recommendations_notebook(customer_id):
#     top_products = hybrid_recommend(customer_id, top_n=10)
#     matched = match_images_to_products(top_products)

#     rows, cols = 2, 5
#     html = "<table style='width:100%; table-layout: fixed;'>"

#     for i in range(rows):
#         html += "<tr>"
#         for j in range(cols):
#             idx = i * cols + j
#             if idx >= len(matched):
#                 html += "<td></td>"
#                 continue
#             item = matched[idx]
#             product_name = item["Product_Name"]
#             img_url = item.get("Matched_Image_URL")


#             if img_url:
#                 try:
#                     response = requests.get(img_url, timeout=5)
#                     img = Image.open(BytesIO(response.content))
#                     img.thumbnail((150,150))
#                     buffer = BytesIO()
#                     img.save(buffer, format="PNG")
#                     encoded_img = buffer.getvalue()
#                     # Convert to base64 for HTML
#                     import base64
#                     img_base64 = base64.b64encode(encoded_img).decode('utf-8')
#                     img_html = f"<img src='data:image/png;base64,{img_base64}' style='width:150px;'><br>{product_name}"
#                     html += f"<td align='center'>{img_html}</td>"
#                 except:
#                     html += f"<td align='center'>{product_name}<br>No image</td>"
#             else:
#                 html += f"<td align='center'>{product_name}<br>No image</td>"
#         html += "</tr>"
#     html += "</table>"

#     display(HTML(html))

def get_recommendations_image_pair(customer_id, top_n=10):
    """
    Returns top-N recommended products with matched image URLs.
    Handles exact/fuzzy matching + keyword fallback.
    Output: list of dicts -> [{"Product_Name": ..., "Image_URL": ...}, ...]
    """
    # 1️⃣ Get recommended products
    top_products = hybrid_recommend(customer_id, top_n=top_n)

    # 2️⃣ Load sitemap Excel and extract URLs + filenames
    sitemap_df = pd.read_excel("afrimash_image_map.xlsx")  # sitemap Excel
    sitemap_df = sitemap_df.dropna(subset=["Image_URL"])
    image_urls = sitemap_df["Image_URL"].tolist()
    image_filenames = [url.split("/")[-1] for url in image_urls]

    # Build a quick lookup dict: {filename: full_url}
    filename_to_url = {fname: url for fname, url in zip(image_filenames, image_urls)}

    # 3️⃣ Build fallback keyword map
    keywords = ["broiler", "pullet", "duckling", "turkey", "papaya",
                "maize", "snail", "fertilizer", "coccidiosis"]
    keyword_image_map = {}
    for kw in keywords:
        matches = [fname for fname in image_filenames if kw in fname.lower()]
        if matches:
            keyword_image_map[kw] = matches

    # 4️⃣ Match images to products
    matched = []
    for prod in top_products:
        # Fuzzy match first
        best_match = process.extractOne(prod, image_filenames, score_cutoff=70)
        if best_match:
            filename = best_match[0]
            image_url = filename_to_url.get(filename)
        else:
            # Keyword fallback
            image_url = None
            for kw in keywords:
                if kw in prod.lower() and kw in keyword_image_map:
                    fallback_filename = random.choice(keyword_image_map[kw])
                    image_url = filename_to_url.get(fallback_filename)
                    break

        matched.append({
            "Product_Name": prod,
            "Image_URL": image_url
        })

    return matched


# def get_recommendations_image_pair(customer_id, top_n=10):
#     """
#     Returns top-N recommended products with matched image filenames.
#     Handles exact/fuzzy matching + keyword fallback.
#     Output: list of dicts -> [{"Product_Name": ..., "Image_Filename": ...}, ...]
#     """
#     # 1️⃣ Get recommended products
#     top_products = hybrid_recommend(customer_id, top_n=top_n)

#     # 2️⃣ Prepare image lookup
#     sitemap_df = pd.read_excel("afrimash_image_map.xlsx")  # sitemap Excel
#     image_urls = sitemap_df['Image_URL'].dropna().tolist()
#     image_filenames = [url.split('/')[-1] for url in image_urls]

#     # 3️⃣ Build fallback keyword map
#     keywords = ["broiler", "pullet", "duckling", "turkey", "papaya",
#                 "maize", "snail", "fertilizer", "coccidiosis"]
#     keyword_image_map = {}
#     for kw in keywords:
#         matches = [fname for fname in image_filenames if kw in fname.lower()]
#         if matches:
#             keyword_image_map[kw] = matches

#     # 4️⃣ Match images to products
#     matched = []
#     for prod in top_products:
#         # Fuzzy match first
#         best_match = process.extractOne(prod, image_filenames, score_cutoff=70)
#         if best_match:
#             idx = image_filenames.index(best_match[0])
#             matched.append({
#                 "Product_Name": prod,
#                 "Image_Filename": image_filenames[idx]
#             })
#         else:
#             # Keyword fallback
#             fallback_filename = None
#             for kw in keywords:
#                 if kw in prod.lower() and kw in keyword_image_map:
#                     fallback_filename = random.choice(keyword_image_map[kw])
#                     break
#             matched.append({
#                 "Product_Name": prod,
#                 "Image_Filename": fallback_filename
#             })

#     return matched

# EXAMPLE USAGE
# ===============================
# Suppose `top_products` is the output from hybrid_recommend(customer_id)
# display_recommendations_notebook("CUS101143")
get_recommendations_image_pair("CUS101143")
